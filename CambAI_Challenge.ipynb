{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6956f582",
   "metadata": {},
   "source": [
    "# Data Engineering Pipeline for Creating a TTS Dataset for the Irish Language from YouTube\n",
    "\n",
    "#### Data Science Position Challenge: \n",
    "Design and implement a data engineering pipeline for creating a Text-to-Speech (TTS) dataset for the Irish language from Youtube. Your submission should include a comprehensive document outlining your proposed pipeline, code, and a subset of the collected dataset formatted similarly to LibriTTS (yes we want you to actually execute your data pipeline and collect 1 Gb of data as representative sample of your pipeline actually working), showcasing your approach to data collection, processing, and management.\n",
    "\n",
    "\n",
    "#### Introduction\n",
    "The aim of this project was to construct a data engineering pipeline dedicated to generating a Text-to-Speech (TTS) dataset for the Irish language, utilizing the rich array of content available on YouTube. This pipeline was specifically designed to automate the extraction, processing, and formatting of audio and textual data to adhere to LibriTTS specifications.\n",
    "\n",
    "#### Approach and Design Decisions\n",
    "\n",
    "1. **Data Collection & Filtering**: Initially, [YouTube Data API V3](https://developers.google.com/youtube/v3) method for data collection was explored as documented in the \"Extracting Irish Channel IDs and Videos using YT Data API V3\" section of this notebook. However, due to the API rate limits and the inaccuracy of transcription for the Irish language, these methods proved unsatisfactory. Consequently, the project shifted towards utilizing `yt-dlp`, a command-line tool that surpassed the YouTube Data API's limitations, facilitating more extensive and flexible data collection.\n",
    "\n",
    "2. **Transcript Extraction and Filtering**: After experimenting with different approaches, the pipeline successfully implemented a method bypassing the limitations encountered with the YouTube Data API and Google Cloud's Speech-to-Text API as mention in the section \"Extracting Captions/ Text\". YT Data API V3 support extracting captions ONLY for videos which the user is authorized to. I also tried extracting audio first and then using Speech-to-Text API to extract text from the audio, however that is not accurate. The final approach utilized `YouTubeTranscriptApi` to directly extract transcripts, ensuring a more reliable and focused retrieval of Irish language transcripts.\n",
    "\n",
    "3. **Audio Extraction**: Leveraging `yt-dlp`, the pipeline efficiently extracted audio tracks from downloaded YouTube videos. This tool was chosen for its robustness and compatibility with a wide range of formats, ensuring the audio data maintained a consistent standard necessary for TTS applications.\n",
    "\n",
    "4. **Data Processing and Formatting**: Following the extraction, audio files were aligned with their respective transcripts, and the data was organized following the structure of the LibriTTS dataset. This involved detailed formatting to ensure compatibility with TTS systems, highlighting the necessity of precise alignment and standardized data representation.\n",
    "\n",
    "5. **LibriTTS Format Creation**: In adherence to LibriTTS standards, the dataset was meticulously organized into structured directories, each containing segmented trasncript files and their corresponding audio and JSON and Text captions/transcriptions. This structure is essential for TTS model training, facilitating the easy location and association of text with spoken audio.\n",
    "\n",
    "6. **Data Quality and Packaging**: The final phase involved conducting thorough quality checks to validate the dataset's integrity and usability. The completed dataset was then packaged, ready for distribution and utilization in TTS systems, ensuring a focus on the Irish language.\n",
    "\n",
    "#### Challenges and Solutions\n",
    "Navigating API limitations was a significant challenge, resolved by transitioning to `yt-dlp` for video collection and direct transcript extraction. This adaptation was crucial in broadening the dataset and ensuring the inclusion of authentic Irish language content.\n",
    "\n",
    "Another notable challenge was the accurate extraction and verification of Irish language transcripts. By opting for direct transcript fetching and setting stringent filtering criteria, the pipeline enhanced the relevance and quality of the textual data, crucial for a nuanced language such as Irish.\n",
    "\n",
    "#### Conclusion\n",
    "This pipeline underscores the innovative use of online video content for linguistic resource development, particularly for underrepresented languages. While tailored for the Irish language, its adaptable framework can serve other languages and dialects. The project encapsulates a holistic approach to TTS dataset creation, from strategic data collection to meticulous formatting in line with LibriTTS standards, marking a significant stride in the realm of language technology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb08e7",
   "metadata": {},
   "source": [
    "# Designing LibriTTS Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2cf5a2",
   "metadata": {},
   "source": [
    "### Adaptation for Our Dataset\n",
    "\n",
    "Due to the unique nature of our dataset, which consists of a single continuous audio file rather than individual clips for each transcript segment, and the lack of distinct speaker information, I have modified the conventional LibriTTS format to accommodate our specific requirements. This adaptation maintains the organizational structure of the original format while addressing the unique aspects of our data.\n",
    "\n",
    "### Directory and File Structure\n",
    "\n",
    "The dataset is organized to reflect a single, continuous audio context. Files are organized based on a unique identifier, which in our case is the `Video_ID`, corresponding to the time of the dataset's creation or recording. This structure accommodates our continuous audio file and segmented transcript references efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "**Directory Structure Example: `LibriTTS-Irish/`**\n",
    "\n",
    "```plaintext\n",
    "LibriTTS-Irish/\n",
    "    V-Hmc-MRRiU/                # Unique Video ID directory\n",
    "        V-Hmc-MRRiU.wav         # The single, continuous audio file\n",
    "        segments/               # Directory for text segment files\n",
    "            0000_V-Hmc-MRRiU_09700.txt          # Text file for a specific segment\n",
    "            0000_V-Hmc-MRRiU_12440.txt          # Another segment text file\n",
    "            ...\n",
    "        V-Hmc-MRRiU.json        # Complete JSON transcript\n",
    "        V-Hmc-MRRiU.txt         # Complete text transcript\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**File Naming Conventions:**\n",
    "\n",
    "- `V-Hmc-MRRiU` - Represents the unique `Video_ID`.\n",
    "- `0000_V-Hmc-MRRiU_09700`: Here, `0000` is an arbitrary number reflecting a speaker ID (used when specific speaker information is absent),`V-Hmc-MRRiU` is the video_id, and `09700` represents the start time of the segment in milliseconds.\n",
    "- `V-Hmc-MRRiU.wav`: This is the single audio file in .wav format.\n",
    "- `V-Hmc-MRRiU.json` and `V-Hmc-MRRiU.txt`: These are the transcript files for the entire audio, provided in two different formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b80bbc",
   "metadata": {},
   "source": [
    "### Text Files and Time Stamps:\n",
    "\n",
    "For each segment, create a text file in the `segments` directory. These files should contain the transcript text and the corresponding time range from the continuous audio file.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- **File Name:** `0000_V-Hmc-MRRiU_09700.txt`\n",
    "- **File Content:** This includes the transcript segment from the audio.\n",
    "\n",
    "  ```\n",
    "  Dia dhaoibh ar fad, a chairde Gaeil.\n",
    "  ```\n",
    "\n",
    "- **Associated JSON Transcript File (`V-Hmc-MRRiU.json`):** This file includes text segments, start times, and durations. Below is an example structure:\n",
    "\n",
    "  ```json\n",
    "  [\n",
    "      {\n",
    "        \"text\": \"Dia dhaoibh ar fad, a chairde Gaeil.\",\n",
    "        \"start\": 9.7,\n",
    "        \"duration\": 2.0\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"Is c\\u00fais \\u00e1thais dom beannachta\\u00ed na hAthbhliana a ghu\\u00ed oraibh,\",\n",
    "        \"start\": 12.24,\n",
    "        \"duration\": 2.88\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"ar bhur dteaghlaigh, agus ar ghach uile dhuine at\\u00e1 bailithe le ch\\u00e9ile anocht ar O\\u00edche Chinn Bhliana.\",\n",
    "        \"start\": 15.26,\n",
    "        \"duration\": 5.96\n",
    "      },\n",
    "      {\n",
    "        \"text\": \"T\\u00e1im an-sh\\u00e1sta t\\u00fas a chur le Bliain na Gaeilge,\",\n",
    "        \"start\": 22.26,\n",
    "        \"duration\": 3.16\n",
    "      },\n",
    "    ...\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "- **Associated Text Transcript File (`V-Hmc-MRRiU.txt`):** This file includes the full transcript corresponding to the audio file segments:\n",
    "\n",
    "  ```\n",
    "    Dia dhaoibh ar fad, a chairde Gaeil.\n",
    "    Is cúis áthais dom beannachtaí na hAthbhliana a ghuí oraibh,\n",
    "    ar bhur dteaghlaigh, agus ar ghach uile dhuine atá bailithe le chéile anocht ar Oíche Chinn Bhliana.\n",
    "    Táim an-shásta tús a chur le Bliain na Gaeilge,\n",
    "    bliain ina ndéanfar ceiliúradh ar 125 bliain d'athbheochan na Gaeilge in Éirinn.\n",
    "    Ar ndóigh, bhí ról nach beag ag Uachtaráin na hÉireann i gcur chun cinn na Gaeilge\n",
    "    – chomh fada siar leis an gcéad Uachtarán, Dubhghlas de hÍde,\n",
    "    ceannródaí na hathbheochana, a bhí mar chathaoirleach ag an gcruinniú, i mí Iúil 1893\n",
    "    nuair a bunaíodh Conradh na Gaeilge.\n",
    "    Is é an chuspóir a leag de hÍde, Eoin Mac Néill, agus a chomhleacaithe amach dóibh fhéin\n",
    "    nuair a chuireadar tús le hobair pobal-bhunaithe na hAthbheochana, ná an Ghaeilge a choinneáil “dá labhairt in Éirinn”.\n",
    "  ...\n",
    "  ```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362653e",
   "metadata": {},
   "source": [
    "# Code implementation with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2bc0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import io\n",
    "import os\n",
    "import html\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from youtube_transcript_api.formatters import JSONFormatter\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b17b9a",
   "metadata": {},
   "source": [
    "## Extracting Irish Channel IDs and Videos using YT Data API V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61165247",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyAHwxmd1IJS37O4vdurw52tdTMZ39kYQnU'\n",
    "\n",
    "keyword_list = [\n",
    "    'Gaeilge le haghaidh', 'Foghlaim Gaeilge', \n",
    "#     'Cúrsa Gaeilge', 'Nuacht Gaeilge', 'Stair Éireann',\n",
    "#     'Gaeilge Labhartha', 'Bunús na Gaeilge', 'Gaeilge Bheo', 'Cultúr na hÉireann', 'Gaeilge Coláiste',\n",
    "#     'Gaeilge Scoile', 'Éireannach', 'TG4 Gaeilge', 'RTÉ Gaeilge', 'Pobal Gaeilge', 'Gaeilge Aclaí',\n",
    "#     'Gaeilge Físeán', 'Ceachtanna Gaeilge', 'Gaeilge Amhrán'  # 'Gaeilge Amhrán' cautiously included, can skip if music is often returned\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2177469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_irish_channels(api_key, keyword_list, max_results_per_keyword=10):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    all_channels = set()  # Use a set to avoid duplicates\n",
    "\n",
    "    for keyword in keyword_list:\n",
    "        try:\n",
    "            search_response = youtube.search().list(\n",
    "                q=keyword,\n",
    "                part='id,snippet',\n",
    "                maxResults=max_results_per_keyword,\n",
    "                type='channel',\n",
    "                regionCode='IE'\n",
    "            ).execute()\n",
    "\n",
    "            for search_result in search_response.get('items', []):\n",
    "                if search_result['id']['kind'] == 'youtube#channel':\n",
    "                    all_channels.add(search_result['id']['channelId'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with keyword '{keyword}': {e}\")\n",
    "\n",
    "    return list(all_channels)  # Convert the set back to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf11d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "irish_channels = find_irish_channels(API_KEY, keyword_list, max_results_per_keyword=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb72634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(irish_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd715bb",
   "metadata": {},
   "source": [
    "**Here, I initially extracted Channel IDs specific to the Irish region, followed by the extraction of videos from these channels. This approach was adopted to ensure the procurement of videos exclusively in the Irish language, thereby eliminating content featuring a mixture of languages, such as English and Irish.**\n",
    "\n",
    "**Additional Irish Channel IDs can be incorporated manually, or the search criteria can be expanded to identify more Channel IDs, consequently increasing the collection of Video URLs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18798f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "def youtube_search(keywords, api_key, max_results=10, channel_ids=None):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    videos = []\n",
    "    videoids = []\n",
    "    try:\n",
    "        for channel_id in (channel_ids or ['']):  # Iterate through channels if provided, else search broadly\n",
    "            search_response = youtube.search().list(\n",
    "                q=keywords,\n",
    "                part='id,snippet',\n",
    "                maxResults=max_results,\n",
    "                type='video',\n",
    "                regionCode='IE',\n",
    "                videoCaption='closedCaption',  # Ensure videos have closed captions\n",
    "                channelId=channel_id  # Search within a specific channel if provided\n",
    "            ).execute()\n",
    "\n",
    "            for search_result in search_response.get('items', []):\n",
    "                if search_result['id']['kind'] == 'youtube#video':\n",
    "                    videoids.append(search_result['id']['videoId'])\n",
    "                    videos.append(f\"https://www.youtube.com/watch?v={search_result['id']['videoId']}\")\n",
    "                    \n",
    "            if not channel_ids:  # Break early if not searching specific channels\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return videos\n",
    "\n",
    "    return videoids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc26fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "irish_videos = youtube_search(keyword_list, API_KEY, 10, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81678e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gpae1kmFtV8',\n",
       " 'oLuinmqHZf4',\n",
       " '5OdhIXAAjbw',\n",
       " 'yeMzwtNe0tU',\n",
       " 'vzlIxabdzR4',\n",
       " 'VdNIpSZwZ_0',\n",
       " 'ZIfkUPQszPU',\n",
       " 'LIghiqb4rpI',\n",
       " '9W6l_u8ef34',\n",
       " 'jEBBKS5sn6E']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irish_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f47d5",
   "metadata": {},
   "source": [
    "### Final Method Implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "807cfeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_videos(keywords, ytdlp_path, region_code, max_results):\n",
    "    \"\"\"Search for videos on YouTube based on keywords.\"\"\"\n",
    "    video_ids = []\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        # Added '--geo-bypass-country' with 'IE' to simulate access from Ireland\n",
    "        command = [ytdlp_path, '--geo-bypass-country', region_code, '--get-id', f'ytsearch{max_results}:{keyword}']\n",
    "        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if result.stdout:\n",
    "            # Extend the video_ids list with the new ids found\n",
    "            video_ids.extend(result.stdout.strip().split('\\n'))\n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7af1d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V-Hmc-MRRiU', 'LpVMYXWhErU', '3eC2XbN1-Bg', 'cQsm9JPe794', 'TZATuiLuTvQ', 'FXL2aE5uUXc', 'XQdGShFcDaQ', 'Q8UhNJw3bWE', 'TIb0FhBC1QM', 'PXhDlYREJyo', 'fs5fqq2YumA', 'i4ATd-q_YZM', 'uPqoKpyhzP8', 'CUM9T5K8lrI', 'sQgvS2A8Xz8', 'lDxkKsnB7cg', 'V3jUiApohGQ', 'CnrQmSIkP1E', 'wraRj_ch2rU', 'lP1FXSktMtg']\n"
     ]
    }
   ],
   "source": [
    "ytdlp_path = '/Users/nikhiljindal/opt/anaconda3/envs/myenv/bin/yt-dlp'\n",
    "ffmpeg_path = '/opt/homebrew/bin/ffmpeg'\n",
    "region_code = 'IE'\n",
    "max_results = 10\n",
    "video_ids = search_videos(keyword_list,ytdlp_path,region_code, max_results)\n",
    "print(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b715e289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6e6b2",
   "metadata": {},
   "source": [
    "**The Video IDs extracted above have been filtered based on location and keywords. However, it is important to note that not all videos are in the Irish language, as the yt-dlp tool lacks a specific language filtering feature. In the subsequent section, I will address this issue.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e7a82",
   "metadata": {},
   "source": [
    "## Extracting Captions/ Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87faf3bd",
   "metadata": {},
   "source": [
    "### Method 1: Using YT Data API V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e29c6781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authenticated_service():\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token1.json', scopes=['https://www.googleapis.com/auth/youtube.force-ssl'])\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client_secrets.json', scopes=['https://www.googleapis.com/auth/youtube.force-ssl'])\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "    \n",
    "    return build('youtube', 'v3', credentials=creds)\n",
    "\n",
    "# Use this authenticated service to make API calls\n",
    "youtube = get_authenticated_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f37b2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_captions(youtube, video_id, language='en'):\n",
    "    try:\n",
    "        # Retrieve a list of available caption tracks\n",
    "        caption_list = youtube.captions().list(part='snippet', videoId=video_id).execute()\n",
    "        print(caption_list)\n",
    "        caption_id = None\n",
    "        for item in caption_list.get('items', []):\n",
    "            if item['snippet']['language'] == language and item['snippet']['trackKind'] != 'ASR':\n",
    "                caption_id = item['id']\n",
    "                break\n",
    "\n",
    "        if caption_id:\n",
    "            # Download the caption track\n",
    "            request = youtube.captions().download(id=caption_id)\n",
    "            fh = io.FileIO(f\"{video_id}.srt\", mode='wb')\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                _, done = downloader.next_chunk()\n",
    "            fh.close()  # Ensure the file is closed after download\n",
    "            print(f\"Downloaded captions for {video_id}\")\n",
    "            return f\"{video_id}.srt\"\n",
    "        else:\n",
    "            print(f\"No suitable caption track found for {video_id}\")\n",
    "            return None\n",
    "    except HttpError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3fa18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind': 'youtube#captionListResponse', 'etag': 'aDeprX_OcnL6B7s1iXY9xARqB34', 'items': [{'kind': 'youtube#caption', 'etag': 'LLr_yKS8UqCg2tHofDhw2YLW8U8', 'id': 'AUieDaaeetUJNobcsPOKKTlO7gjyISnMbN26WnAoPh8sQdSKU1I', 'snippet': {'videoId': 'jBN2_YuTclU', 'lastUpdated': '2023-11-13T21:21:27.926775Z', 'trackKind': 'asr', 'language': 'en', 'name': '', 'audioTrackType': 'unknown', 'isCC': False, 'isLarge': False, 'isEasyReader': False, 'isDraft': False, 'isAutoSynced': False, 'status': 'serving'}}]}\n",
      "An error occurred: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/captions/AUieDaaeetUJNobcsPOKKTlO7gjyISnMbN26WnAoPh8sQdSKU1I? returned \"The permissions associated with the request are not sufficient to download the caption track. The request might not be properly authorized, or the video order might not have enabled third-party contributions for this caption.\". Details: \"[{'message': 'The permissions associated with the request are not sufficient to download the caption track. The request might not be properly authorized, or the video order might not have enabled third-party contributions for this caption.', 'domain': 'youtube.caption', 'reason': 'forbidden', 'location': 'id', 'locationType': 'parameter'}]\">\n"
     ]
    }
   ],
   "source": [
    "download_captions(youtube, 'jBN2_YuTclU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fecd67",
   "metadata": {},
   "source": [
    "**The method previously described will not function due to modifications in the YouTube DATA API. Please consult the following articles for further information.**\n",
    "\n",
    "- https://issuetracker.google.com/issues/241669016?pli=1\n",
    "- https://www.reddit.com/r/GoogleAppsScript/comments/186e4ot/is_it_even_possible_to_download_any_youtube_video/\n",
    "- https://stackoverflow.com/questions/30653865/downloading-captions-always-returns-a-403"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5481ba1",
   "metadata": {},
   "source": [
    "### Method 2: Using Google Cloud Speech to Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969a6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure you have your correct path to the credentials file\n",
    "# credentials = service_account.Credentials.from_service_account_file('camb_StoT.json')\n",
    "\n",
    "# def convert_to_mono(stereo_file_path):\n",
    "#     sound = AudioSegment.from_wav(stereo_file_path)\n",
    "#     mono_sound = sound.set_channels(1)\n",
    "#     mono_sound.export(stereo_file_path, format=\"wav\")\n",
    "\n",
    "# def transcribe_audio(speech_file, lang=\"ga-IE\"):\n",
    "#     # Convert stereo audio to mono\n",
    "#     convert_to_mono(speech_file)\n",
    "\n",
    "#     # Check the file size after conversion\n",
    "#     file_size = os.path.getsize(speech_file)\n",
    "#     if file_size > 10485760:  # File size greater than 10 MB\n",
    "#         print(f\"Skipping large file: {speech_file} (Size: {file_size} bytes)\")\n",
    "#         return None\n",
    "\n",
    "#     client = speech.SpeechClient(credentials=credentials)\n",
    "#     with io.open(speech_file, 'rb') as audio_file:\n",
    "#         content = audio_file.read()\n",
    "\n",
    "#     audio = speech.RecognitionAudio(content=content)\n",
    "#     config = speech.RecognitionConfig(\n",
    "#         encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "#         sample_rate_hertz=48000,  # Adjust based on your actual files\n",
    "#         language_code=lang,\n",
    "#         enable_word_time_offsets=True\n",
    "#     )\n",
    "\n",
    "#     operation = client.long_running_recognize(config=config, audio=audio)\n",
    "#     print('Waiting for operation to complete...')\n",
    "#     response = operation.result(timeout=90)  # Increase timeout as needed\n",
    "\n",
    "#     for result in response.results:\n",
    "#         print(\"Transcript: {}\".format(result.alternatives[0].transcript))\n",
    "\n",
    "# audio_path = 'data/uG1mplJey4M.wav'\n",
    "# transcribe_audio(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666af6f2",
   "metadata": {},
   "source": [
    "**This method lacks precision and has been observed to produce incorrect English text instead of Irish.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6effe718",
   "metadata": {},
   "source": [
    "### Final Method Implemented: Using YT transcript API \n",
    "https://pypi.org/project/youtube-transcript-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bd94aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d809ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transcript_text_files(video_ids, main_output_directory, language):\n",
    "    irish_video_ids = []\n",
    "    skip = 0\n",
    "    for video_id in video_ids:\n",
    "        output_directory = os.path.join(main_output_directory, video_id)\n",
    "        \n",
    "        # Initialize the path for the transcript file without creating the directory yet\n",
    "        output_file_path = os.path.join(output_directory, f\"{video_id}.txt\")\n",
    "\n",
    "        # Check if the transcript file already exists and is not empty\n",
    "        if os.path.exists(output_file_path) and os.path.getsize(output_file_path) > 0:\n",
    "            print(f\"Text Transcript file already exists and is not empty for {video_id}\")\n",
    "            continue  # Skip to the next file\n",
    "\n",
    "        try:\n",
    "            # Attempt to fetch the transcript in the Irish language\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[language])\n",
    "            if transcript:\n",
    "                # If the transcript exists, create the directory (if it doesn't exist already)\n",
    "                if not os.path.exists(output_directory):\n",
    "                    os.makedirs(output_directory)\n",
    "                \n",
    "                formatter = TextFormatter()\n",
    "\n",
    "                # .format_transcript(transcript) turns the transcript into a text string.\n",
    "                text_formatted = formatter.format_transcript(transcript)\n",
    "\n",
    "                # Write the formatted text content to a file\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(text_formatted)\n",
    "                print(f\"Text Transcript file created for video id: {video_id}\")\n",
    "                irish_video_ids.append(video_id)\n",
    "        except Exception as e:\n",
    "            skip += 1\n",
    "#             print(f\"An error occurred for videos_id {video_id}: {e}\")\n",
    "            # If there's an error, and directory was created without files, remove it\n",
    "            if os.path.exists(output_directory) and not os.listdir(output_directory):\n",
    "                os.rmdir(output_directory)\n",
    "            \n",
    "    print(f\"Skip Counter: {skip}\")        \n",
    "    return irish_video_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0c252aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Transcript file created for video id: V-Hmc-MRRiU\n",
      "Skip Counter: 19\n"
     ]
    }
   ],
   "source": [
    "output_directory = \"Test/\"\n",
    "language = 'ga'\n",
    "irish_video_ids = create_transcript_text_files(video_ids,output_directory,language )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e02ac86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V-Hmc-MRRiU',\n",
       " 'LpVMYXWhErU',\n",
       " '3eC2XbN1-Bg',\n",
       " 'cQsm9JPe794',\n",
       " 'TZATuiLuTvQ',\n",
       " 'FXL2aE5uUXc',\n",
       " 'XQdGShFcDaQ',\n",
       " 'Q8UhNJw3bWE',\n",
       " 'TIb0FhBC1QM',\n",
       " 'PXhDlYREJyo',\n",
       " 'fs5fqq2YumA',\n",
       " 'i4ATd-q_YZM',\n",
       " 'uPqoKpyhzP8',\n",
       " 'CUM9T5K8lrI',\n",
       " 'sQgvS2A8Xz8',\n",
       " 'lDxkKsnB7cg',\n",
       " 'V3jUiApohGQ',\n",
       " 'CnrQmSIkP1E',\n",
       " 'wraRj_ch2rU',\n",
       " 'lP1FXSktMtg']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a05875dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V-Hmc-MRRiU']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irish_video_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532a52d",
   "metadata": {},
   "source": [
    "**The function described above is designed to extract transcripts and filter video IDs based on the availability of Irish captions/transcripts. It should be noted that among 20 video IDs, only one contains Irish captions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c893b2",
   "metadata": {},
   "source": [
    "## Extracting Audio from YT URLS using YT-DLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dd830ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytdlp_path = '/Users/nikhiljindal/opt/anaconda3/envs/myenv/bin/yt-dlp'\n",
    "# ffmeg_path = '/opt/homebrew/bin/ffmpeg'\n",
    "# output_directory='/dataset/video_id/'\n",
    "\n",
    "def download_audio_with_ytdlp(irish_video_ids, ytdlp_path, ffmeg_path, main_output_directory, output_format='wav', sample_rate=24000):\n",
    "    \n",
    "    for video_id in irish_video_ids:\n",
    "        # Define output filename format\n",
    "        \n",
    "        output_directory = main_output_directory + video_id\n",
    "        \n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "        \n",
    "        output_filepath = os.path.join(output_directory, f'{video_id}.{output_format}')\n",
    "        temp_filepath = os.path.join(output_directory, f'temp_{video_id}.{output_format}')\n",
    "\n",
    "        # Check if the audio file already exists to avoid re-downloading\n",
    "        if not os.path.exists(output_filepath):\n",
    "            # Run yt-dlp to download the best audio\n",
    "            yt_dlp_command = [\n",
    "                ytdlp_path,  # Make sure yt-dlp is installed and in your PATH\n",
    "                '--extract-audio',\n",
    "                '--audio-format', output_format,\n",
    "                '--audio-quality', '0',  # Best quality\n",
    "                '--output', f'{output_directory}/%(id)s.%(ext)s',\n",
    "                video_id\n",
    "            ]\n",
    "            subprocess.run(yt_dlp_command, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "\n",
    "            # Convert downloaded audio to desired sample rate and mono channel\n",
    "            ffmpeg_command = [\n",
    "                ffmeg_path,\n",
    "                '-i', output_filepath,  # Input file\n",
    "                '-ac', '1',  # Mono channel\n",
    "                '-ar', str(sample_rate),  # Sample rate\n",
    "                '-y',  # Overwrite output file if it exists\n",
    "                temp_filepath  # Temporary output file\n",
    "            ]\n",
    "            subprocess.run(ffmpeg_command, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "\n",
    "            # Move the converted file back to the original file path\n",
    "            shutil.move(temp_filepath, output_filepath)\n",
    "            print(f\"Audio saved as {output_filepath}\")\n",
    "        else:\n",
    "            print(f\"Audio file already exists: {output_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9542c8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved as Test/V-Hmc-MRRiU/V-Hmc-MRRiU.wav\n"
     ]
    }
   ],
   "source": [
    "download_audio_with_ytdlp(irish_video_ids, ytdlp_path, ffmpeg_path, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06da3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WAS THE CODE FOR METHOD 2 (Speech to Text) OF \"Extracting Captions/ Text\", THAT DIDN'T WORK OUT \n",
    "\n",
    "# import os\n",
    "# from pydub import AudioSegment\n",
    "# import subprocess\n",
    "\n",
    "# def download_audio_with_ytdlp(youtube_url, output_format='wav', sample_rate=24000):\n",
    "#     # Define output filename format\n",
    "#     output_template = 'data/' + '%(id)s.%(ext)s'\n",
    "#     download_path = os.path.join('.', output_template)\n",
    "    \n",
    "#     # Run yt-dlp to download the best audio\n",
    "#     command = [\n",
    "#         '/Users/nikhiljindal/opt/anaconda3/envs/myenv/bin/yt-dlp',\n",
    "#         '--extract-audio',\n",
    "#         '--audio-format', output_format,\n",
    "#         '--output', download_path,\n",
    "#         youtube_url\n",
    "#     ]\n",
    "#     subprocess.run(command, check=True)\n",
    "\n",
    "#     # Find downloaded file (assuming yt-dlp names files correctly)\n",
    "#     downloaded_files = [f for f in os.listdir('.') if f.endswith('.wav')]\n",
    "#     if downloaded_files:\n",
    "#         downloaded_file = downloaded_files[0]  # Assuming we only downloaded one file\n",
    "#         # Convert audio to desired sample rate and mono channel using ffmpeg\n",
    "#         output_file = f\"{downloaded_file.split('.')[0]}_converted.{output_format}\"\n",
    "#         ffmpeg_command = [\n",
    "#             'ffmpeg',\n",
    "#             '-i', downloaded_file,\n",
    "#             '-ac', '1',  # Set audio channels to 1 (mono)\n",
    "#             '-ar', str(sample_rate),  # Set sample rate to 24 kHz\n",
    "#             '-y',  # Overwrite output file if exists\n",
    "#             output_file\n",
    "#         ]\n",
    "#         subprocess.run(ffmpeg_command, check=True)\n",
    "\n",
    "#         # Remove the original download to save space, if different\n",
    "#         if downloaded_file != output_file:\n",
    "#             os.remove(downloaded_file)\n",
    "#         return output_file\n",
    "#     else:\n",
    "#         print(\"No audio file downloaded.\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "# for video in videos:\n",
    "#     output_audio_file = download_audio_with_ytdlp(video)\n",
    "#     if output_audio_file:\n",
    "#         print(f\"Audio saved as {output_audio_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9e781",
   "metadata": {},
   "source": [
    "## Data Processing and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0db4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transcript_json_files(irish_video_ids, main_output_directory, language):\n",
    "    \n",
    "    for video_id in irish_video_ids:\n",
    "        \n",
    "        output_directory = main_output_directory + video_id\n",
    "        \n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)        \n",
    "        \n",
    "        output_file_path = os.path.join(output_directory, video_id + '.json')  # Define path for transcript file\n",
    "\n",
    "        # Check if the transcript file already exists and is not empty\n",
    "        if os.path.exists(output_file_path) and os.path.getsize(output_file_path) > 0:\n",
    "            print(f\"JSON Transcript file already exists and is not empty for {video_id}\")\n",
    "            continue  # Skip to the next file\n",
    "\n",
    "        try:\n",
    "            # Attempt to fetch the transcript in the Irish language\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[language])\n",
    "            \n",
    "            formatter = JSONFormatter()\n",
    "\n",
    "            # .format_transcript(transcript) turns the transcript into a JSON string.\n",
    "            json_formatted = formatter.format_transcript(transcript, indent=2)\n",
    "            \n",
    "            # Write the formatted text content to a file\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(json_formatted)\n",
    "            print(f\"Text Transcript file created for video id: {video_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for video_id {video_id}: {e}\")\n",
    "            # Skip file if there's an error (you can decide to delete the audio file or keep it for manual review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfa660cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_files(main_directory, speaker_id='0000'):\n",
    "    # Iterate over all subdirectories in the main directory\n",
    "    for subdir in next(os.walk(main_directory))[1]:\n",
    "        video_id = subdir\n",
    "        json_path = Path(main_directory) / video_id / f\"{video_id}.json\"\n",
    "        output_dir = Path(main_directory) / video_id / 'segments'\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load the transcript data from the JSON file if it exists\n",
    "        if json_path.exists():\n",
    "            with open(json_path, 'r', encoding='utf-8') as file:\n",
    "                transcripts = json.load(file)\n",
    "\n",
    "            # Iterate over the transcript entries and create a text file for each\n",
    "            for entry in transcripts:\n",
    "                # Construct the filename using the speaker ID and start time as unique identifier\n",
    "                start_time_str = \"{:05d}\".format(int(entry['start'] * 1000))  # Convert start time to milliseconds and format\n",
    "                filename = f\"{speaker_id}_{video_id}_{start_time_str}.txt\"\n",
    "                file_path = output_dir / filename\n",
    "\n",
    "                # Write the transcript text to the file\n",
    "                with open(file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(entry['text'])\n",
    "            print(f\"Text files created for video ID: {video_id}\")\n",
    "        else:\n",
    "            print(f\"No JSON found for video ID: {video_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84714345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting JSON transcripts...\n",
      "Text Transcript file created for video id: V-Hmc-MRRiU\n",
      "Preparing Segments...\n",
      "Text files created for video ID: V-Hmc-MRRiU\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting JSON transcripts...\")\n",
    "create_transcript_json_files(irish_video_ids, output_directory, language)\n",
    "    \n",
    "print(\"Preparing Segments...\")\n",
    "create_text_files(output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
